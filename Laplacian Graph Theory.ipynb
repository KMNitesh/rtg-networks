{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>The Graph Laplacian</h1>\n",
    "\n",
    "For any graph, one can define a matrix, the combinatorial Laplacian (Laplacian for short), \n",
    "The Laplacian matrix can be written as \n",
    "$$L = D - W$$\n",
    "where $D$ is the $n \\times n$ diagonal degree matrix and $W$ is the adjacency matrix.\n",
    "It can be seen that\n",
    "$$(L x)_i = \\sum_{j=1}^n W_{i,j} (x_i - x_j)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Proposition 1</h2>\n",
    "Suppose that $\\{C_k\\}_{k=1}^K$ are maximal connected sets of vertices of $G$ (the addition of any vertex to $C_k$ would be disconnected).  Then the null space of $L$, that is all vectors $x \\in \\mathbb{R}^n$ such that $Lx = 0$, is the span of the vectors \n",
    "$$\\mathbf{1}_{C_k}, \\quad k=1,\\ldots,K$$\n",
    "where $\\mathbf{1}_A$ is 1 for the vertices in $A$ and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Lemma 1</h4>\n",
    "For an unweighted, undirected graph $G$ that has only 1 connected component, the null space of its Lapacian $L$ is the span of the vector $\\mathbf{1}$.\n",
    "\n",
    "<h5>Proof</h5>\n",
    "Suppose a graph $G$ has only 1 connected component.  It is immediately evident for $q \\in \\mathbb{R}$ that $span(\\mathbf{1}) = (q\\mathbf{1}) \\subseteq null(L)$, since $$(L (q\\mathbf{1}))_i = \\sum_{j=1}^n W_{i,j} ((q\\mathbf{1})_i - (q\\mathbf{1})_j) = \\sum_{j=1}^n W_{i,j} (q - q) = 0, \\quad i = 1,\\ldots,n.$$\n",
    "Now suppose that there exists a vector $x \\in null(L)$ such that $x$ can not be expressed in the form $q\\mathbf{1}$ where $q \\in \\mathbb{R}$.  Then, $x_i \\ne x_j$ for at least one $i \\ne j$ and $$(L x)_i = \\sum_{j=1}^n W_{i,j} (x_i - x_j) = 0, \\quad i = 1,\\ldots,n.$$  This implies that the quadratic form $$x^TLx = \\sum_{i,j : i > j}W_{i,j}(x_i - x_j)^2 = \\sum_{i,j \\in E}(x_i - x_j)^2 = 0,$$ which can only occur when $x_i = x_j, \\forall i,j$. This contradicts the supposition that $x_i \\ne x_j$ for at least one $i \\ne j$. Therefore, if $x \\in null(L)$ then $x_i = x_j, \\forall i,j \\Longleftrightarrow x \\in span(\\mathbf{1})$.  Thus, $null(L) \\subseteq span(\\mathbf{1})$.\n",
    "\n",
    "Since $null(L) \\subseteq span(\\mathbf{1})$ and $span(\\mathbf{1}) \\subseteq null(L)$, it follows that $null(L) = span(\\mathbf{1})$. $\\Box$\n",
    "\n",
    "<h3>Proof</h3>\n",
    "Let $L_i$ be the the Lapacian of the graph defined by the vertices from $C_i$, where $i = 1, \\ldots, K$. Then, by reording the points, the Lapacian $L^K$ can be written as the block diagonal matrix below\n",
    "\n",
    "$$\n",
    "L^K =\n",
    "\\begin{bmatrix}\n",
    "    L_{1} & 0 & \\dots  & 0 \\\\\n",
    "    0 & L_{2} & \\dots  & 0 \\\\\n",
    "    \\vdots  & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\dots  & L_{K}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Consider the Lapacian $L^1$ of a graph for which there is only one connected component ($K = 1$).  From **Lemma 1**, it follows that $null(L^1) = span(\\mathbf{1}_{C_1})$. \n",
    "\n",
    "Suppose that the null space of the Lapacian $L^p$ for a graph $G$ with $p$ connected components is the span of the vectors $\\mathbf{1}_{C_k}, k=1,\\ldots,p$. Then if we write $L^p$ in the form \n",
    "$$\n",
    "L^p =\n",
    "\\begin{bmatrix}\n",
    "    L_{1} & 0 & \\dots  & 0 \\\\\n",
    "    0 & L_{2} & \\dots  & 0 \\\\\n",
    "    \\vdots  & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\dots  & L_{p}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "then the null space is the span of the vectors\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{1}_{C_1} \\\\\n",
    "    0 \\\\\n",
    "    \\vdots  \\\\\n",
    "    0 \n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    \\mathbf{1}_{C_2} \\\\\n",
    "    \\vdots  \\\\\n",
    "    0 \n",
    "\\end{bmatrix},\n",
    "\\ldots,\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    \\vdots  \\\\\n",
    "    \\mathbf{1}_{C_p} \n",
    "\\end{bmatrix},\n",
    "$$\n",
    "where $\\mathbf{1}_{C_k}$ has the same number of rows as $L_k$ (by construction).\n",
    "Consider the Lapacian $L^{p+1}$ of a graph with one extra connected component. This can be written in the form\n",
    "$$\n",
    "L^{p+1} =\n",
    "\\begin{bmatrix}\n",
    "    L^{p} & 0 \\\\\n",
    "    0 & L_{p+1}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    L_{1} & 0 & \\dots & 0 & 0 \\\\\n",
    "    0 & L_{2} & \\dots & 0 & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "    0 & 0 & \\dots & L_{p}  & 0 \\\\\n",
    "    0 & 0 & \\dots & 0 & L_{p+1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "From **Lemma 1**, we know that the null space of $L_{p+1}$ is the span of $\\mathbf{1}_{C_{p+1}}$. Because the columns and rows from $L_{p+1}$ are linearly independent from the rows and columns of $L^p$, it follows that the null space of $L^{p+1}$ is the span of the vectors\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{1}_{C_1} \\\\\n",
    "    0 \\\\\n",
    "    \\vdots  \\\\\n",
    "    0 \\\\\n",
    "    0\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    \\mathbf{1}_{C_2} \\\\\n",
    "    \\vdots  \\\\\n",
    "    0 \\\\\n",
    "    0\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    \\vdots  \\\\\n",
    "    \\mathbf{1}_{C_p} \\\\\n",
    "    0\n",
    "\\end{bmatrix},\n",
    "\\ldots,\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    \\vdots  \\\\\n",
    "    0 \\\\\n",
    "    \\mathbf{1}_{C_{p+1}}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Therefore, by the Principle of Mathematical Induction, **Proposition 1** is true. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spectral Clustering</h2>\n",
    "\n",
    "A graph $\\mathbf{G} = (\\mathbf{V},\\mathbf{E})$ can be partitioned  or clustered into two disjoint sets $A$ and $B$ (where $A \\cup B = V$ and $A \\cap B = \\emptyset$) by removing edges connecting the two parts. In graph theory, the *cut* of $A$ and $B$ $$cut(A,B) = \\sum_{u \\in A, v \\in B}w(u,v)$$ is a measure of dissimilarity between the two pieces of the graph. In the case of an undirected unweighted graph, the cut is simply the number of edges in $E$ that are removed in forming the partition. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Normalized Cut </h3>\n",
    "As presented by Shi and Malik [*Normalized Cuts and Image Segmentation*](https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf \"Title\") (2000), one should consider the following questions prior to partitioning a graph:\n",
    "1. What is a criterion for a good partition?\n",
    "2. How can such a criterion be committed efficiently?\n",
    "\n",
    "A simple partitioning criterion, the *minimized cut*, claims that the optimal bipartitioning of a graph minimizes the value of $cut(A,B)$.  However, this criterion can be shown to be biased to favor cutting small nodes in isolated portions of the graph. To avoid this unnatural bias, Shi and Malik proposed the **normalized cut**\n",
    "$$Ncut(A,B) = \\frac{cut(A,B)}{assoc(A,V)} + \\frac{cut(A,B)}{assoc(B,V)}$$\n",
    "where $assoc(A,V) = \\sum_{u \\in A, t \\in V}w(u,t)$ is the total connection from nodes in A to all nodes in the graph. The intuition behind this measure is to examine the cut cost as a fraction of the total edge connections to all nodes in the graph. Shi and Malik argued that minimizing the normalized cut is a good criterion for forming a reasonable partition of $G$ because it simultaneously minimized dissociation between the groups and maximized association within the groups. Through a rigorous derivation, Shi and Malik showed that \n",
    "$$min_\\mathbf{x}Ncut(\\mathbf{x}) = min_\\mathbf{y}\\frac{\\mathbf{y}^T\\mathbf{L}\\mathbf{y}}{\\mathbf{y}^T\\mathbf{D}\\mathbf{y}}$$\n",
    "with the conditions that (1) $\\mathbf{y}(i) \\in \\{1,-b\\}$ (for some $b \\in \\mathbb{R}^+$) and (2) $\\mathbf{y}^T\\mathbf{D}\\mathbf{1} = 0$ (or $\\mathbf{y}^T\\mathbf{D}$ is orthogonal to the ones vector). The above expression is a well studied form called the Rayleigh quotient, and therefore, if $\\mathbf{y}$ **is relaxed to take on real values**, then we can minimize the normalized cut above by solving the generalized eigenvalue system\n",
    "$$\\mathbf{L}\\mathbf{y} = \\lambda\\mathbf{D}\\mathbf{y}$$ which we can re-write as \n",
    "$$\\mathbf{D}^{-\\frac{1}{2}}\\mathbf{L}\\mathbf{D}^{-\\frac{1}{2}}\\mathbf{z}= \\lambda\\mathbf{z}$$\n",
    "where $\\mathbf{z} = \\mathbf{D}^{\\frac{1}{2}}\\mathbf{y}$. Because vector $\\mathbf{z}_0 = \\mathbf{D}^{\\frac{1}{2}}\\mathbf{1}$ is an eigenvector with eigenvalue 0 in the lower system, we can see that constraint $\\mathbf{y}^T\\mathbf{D}\\mathbf{1} = 0$ is automatically satisfied by the solution to generalized eigensystem.  Because  $\\mathbf{L}$ is know to be a symmetric (undirected case), positive semidefinite matrix $\\mathbf{z}_0$ has the smallest eigenvalue in the lower system and that all eigenvectors of the lower system are orthogonal. Translating the constraints back into the higher generalized eigensystem we have (1) $\\mathbf{y}_0 = \\mathbf{1}$ is the smallest eigenvector with eigenvalue 0 and (2) $0 = \\mathbf{z}_1^T\\mathbf{z}_0 = \\mathbf{y}_1^T\\mathbf{D} \\mathbf{1}$, where $\\mathbf{z}_1$ and $\\mathbf{y}_1$ are the second smallest eigenvectors of the lower and upper systems respectively. \n",
    "Under constraint (2) (orthogonality of 2 smallest eigenvectors), it can be shown that the Rayleigh quotient is minimized by the 2nd smallest eigenvector, or the Fiedler Vector, and takes the minimum value equal to the eigenvalue of the Fiedler Vector. Therefore, it follows that the second smallest eigenvector $\\mathbf{y}_1$ of the higher generalized eigensystem\n",
    "$$\\mathbf{y}_1 = arg.min_{\\mathbf{y}^T\\mathbf{D}\\mathbf{1} = 0}\\frac{\\mathbf{y}^T\\mathbf{L}\\mathbf{y}}{\\mathbf{y}^T\\mathbf{D}\\mathbf{y}}$$\n",
    "is the real-valued solution to the problem.\n",
    "\n",
    "<h4>Summary of Minimum Normalized Cut Algorithm</h4>\n",
    "In conclusion, the graph partitioning/clustering algorithm using the minimum normalized cut can be broken down into 4 steps:\n",
    "1. Set up an undirected graph $\\mathbf{G} = (\\mathbf{V},\\mathbf{E})$ for which you seek to partition.\n",
    "2. Solve $\\mathbf{L}\\mathbf{y} = \\lambda\\mathbf{D}\\mathbf{y}$ for eigenvectors with the smallest eigenvalues. (Or equivalently solve $\\mathbf{D}^{-\\frac{1}{2}}\\mathbf{L}\\mathbf{D}^{-\\frac{1}{2}}\\mathbf{z}= \\lambda\\mathbf{z}$ and convert back to $\\mathbf{y}$ through the relation $\\mathbf{y} = \\mathbf{D}^{-\\frac{1}{2}}\\mathbf{z}$)\n",
    "3. Use the eigenvector with the second smallest eigenvalue to bipartition the graph. (Often 0 is chosen as a cut-off point, so that all nodes corresponding to negative entries in the second smallest eigenvector belong to one group and all nodes corresponding to postive entries in the second smalllest eigenvector belong to another).\n",
    "4. Decide if current partition should subdivided, then recursively sub-divided the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The Ratio Cut</h3>\n",
    "An alternative to the minimum normalized cut criterion is the minimum ratio cut criterion. Let $|A|$ is the number of nodes in a set $A$. Then the ratio cut is defined as\n",
    "$$Rcut(A,B) = n\\frac{Cut(A,B)}{|A||B|}$$\n",
    "\n",
    "Similar to the normalized cut, it can be shown that \n",
    "$$min_\\mathbf{x}Rcut(\\mathbf{x}) = min_\\mathbf{y}\\frac{\\mathbf{y}^T\\mathbf{L}\\mathbf{y}}{\\mathbf{y}^T\\mathbf{y}}$$\n",
    "under the conditions that (1) $\\mathbf{y}(i) \\in \\{1,-b\\}$ (for some $b \\in \\mathbf{R}^+$) and (2) $\\mathbf{y}^T\\mathbf{1} = 0$ (see [here](http://www.cis.upenn.edu/~cis515/cis515-15-spectral-clust-chap5.pdf) for more).  Because this is a special case of the minimum normalized cut problem with $\\mathbf{D}$ equal to the identity matrix, the real valued solution to our problem is the Fiedler Vector (eigenvector with the second smallest eigenvalue) of the eigensystem\n",
    "$$\\mathbf{L}\\mathbf{y} = \\lambda\\mathbf{y}$$\n",
    "\n",
    "<h4>Summary of Minimum Ratio Cut Algorithm</h4>\n",
    "In conclusion, the graph partitioning/clustering algorithm using the ratio cut can be broken down into 4 steps:\n",
    "1. Set up an undirected graph $\\mathbf{G} = (\\mathbf{V},\\mathbf{E})$ for which you seek to partition.\n",
    "2. Solve $\\mathbf{L}\\mathbf{y} = \\lambda\\mathbf{y}$ for eigenvectors with the smallest eigenvalues.\n",
    "3. Use the eigenvector with the second smallest eigenvalue to bipartition the graph. (Often 0 is chosen as a cut-off point, so that all nodes corresponding to negative entries in the second smallest eigenvector belong to one group and all nodes corresponding to postive entries in the second smalllest eigenvector belong to another).\n",
    "4. Decide if current partition should subdivided, then recursively sub-divided the two groups.\n",
    "\n",
    "**Note**: The only difference between the implimentation of this algorithm and the minimum normalized cut algorithm is that we solve for the small eigenvectors of the unnormalized Lapacian $L$ instead of the normalized Lapacian $D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}}$; and therefore, we don't need to left-multiply the resulting eigenvectors by $D^{-\\frac{1}{2}}$."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
