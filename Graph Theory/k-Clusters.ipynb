{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$K$-Way Spectral Clustering</h1>\n",
    "Often we may wish to determine whether or not $K$ communities are present in a particular graph, and if so, to uncover them efficiently.  Methods similar to Shi and Malik's Normalized Cut algorithm, which use the second smallest eigenvector of the Laplacian to bi-partition a graph, can be used recursively in order to obtain $K$ clusters of vertices. However, if $K$ is not an order of 2, these bi-partitioning methods can be shown to perform poorly. \n",
    "\n",
    "Intuitively, this makes sense. For example, if $K = 3$, then the only \n",
    "\n",
    "Thus, in [On Spectral Clustering: Analysis and an algorithm](http://ai.stanford.edu/~ang/papers/nips01-spectral.pdf \"Title\"), Ng, Jordan, and Weiss sought to devise a practical algorithm that would simultaneously partition a graph $G$ into $K$ parts effectively.\n",
    "\n",
    "<h3>Algorithm</h3>\n",
    "\n",
    "1. Set up an undirected graph $G=(V,E)$ for which you want to cluster into $k$-groups.\n",
    "2. Find $x_1, x_2, \\ldots, x_k$ the $k$ smallest eigenvectors of the Laplacian used (where $x_1, x_2, \\ldots, x_k$ are orthogonal to one another).\n",
    "3. Form the matrix $X = [x_1, x_2, \\ldots, x_k] \\in \\mathbb{R}^{n \\times k}$.\n",
    "4. Form the matrix $Y$ by renormalizing each row of $X$ to have unit length (i.e. $Y_{ij} = X_{ij}/(\\sum_{j}X_{ij})^2$).\n",
    "5. Treat each row of Y as a point in $\\mathbb{R}^k$ and cluster using [K-means clustering](https://sites.google.com/site/dataclusteringalgorithms/k-means-clustering-algorithm \"Title\") (or another algorithm that minimizes distortion).\n",
    "6. Assign point $s_i$ to cluster $j$ if and only if the $i$-th row of $Y$ was assigned to cluster $j$ in section 5.\n",
    "\n",
    "The idea behind using this type of algorithm is intuitive: Recall that when a graph has $K$ disconnected components, $\\{C_k\\}_{k=1}^K$, the smallest $K$ eigenvectors of its Laplacian $L$ will be the span of the vectors \n",
    "$$\\mathbf{1}_{C_k}, \\quad k=1,\\ldots,K$$\n",
    "where $\\mathbf{1}_A$ is 1 for the vertices in $A$ and 0 otherwise. Therefore, under the ideal case where there are $K$ disconnected components in a graph, the transformation $L \\rightarrow Y$ maps of each row of $L$ to one of $K$ mutually orthogonal points on a unit $K$-sphere. Points are a $K$-tuple of the form $1_k$, or are 1 at the $k$-th entry if it is in the $k$-th disconnected component and 0s elsewhere. Using k-means on these points recovers the separate disconnected components (as all points in the same disconnected component are mapped to the same point and are thus infinitely close).\n",
    "\n",
    "If there is only 1 disconnected component, then the transformation $L \\rightarrow Y$ maps each row of $L$ to a point on a unit $K$-sphere. In this case, each row is not mapped to exactly the same point, but using k-means clustering on these points can determine a good clustering of vertices. The intuition here is that if points are close to one another in this reduced $K$-dimensional eigenspace, they are more similar to one another than points farther away.\n",
    "\n",
    "Although not directly maximizing or minimizing any specific criterion, using the Normalized Laplacian (${D}^{-\\frac{1}{2}}{L}{D}^{-\\frac{1}{2}}$) when performing the algorithm tends to balance the number of edges across the $K$ clusters, while using the Combinatorial Laplacian ($L$) tends to balance the number of nodes across the resulting $K$ clusters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
